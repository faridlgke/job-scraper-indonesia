{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc9d9ba",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; color: darkred;\">JOBSTREET CRAWLING</h1>\n",
    "<h3 style=\"text-align:center;\">Created by : Faridl Gifari Kertabudi (fargif991@gmail.com)</h3>\n",
    "<hr style=\"border: 1px solid black;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683b2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bc6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today()\n",
    "today_string = today.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91c95acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = \"E:/Chromedriver/chromedriver-win64/chromedriver.exe\"\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9d8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_link = \"https://www.jobstreet.co.id/id/data-analyst-jobs?\"\n",
    "\n",
    "# Navigate to the webpage containing the element\n",
    "driver.get(web_link)\n",
    "\n",
    "# Refresh the webpage\n",
    "driver.refresh()\n",
    "\n",
    "# Wait for a few seconds to see the effect\n",
    "time.sleep(2)\n",
    "\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bbfb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobid_getter(url):\n",
    "    # Parse the URL\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Extract the query parameters\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "    # Get the currentJobId\n",
    "    current_job_id = query_params.get('jobId', [None])[0]\n",
    "    return current_job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fbbaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_db2 = pd.DataFrame(columns=[\n",
    "    'company'\n",
    "    , 'role'\n",
    "    , 'location'\n",
    "    , 'post_duration'\n",
    "    , 'total_application'\n",
    "    , 'job_description'\n",
    "    , 'level'\n",
    "    , 'employment_status'\n",
    "    , 'job_function'\n",
    "    , 'industries'\n",
    "    , 'url'\n",
    "    , 'job_id'\n",
    "    , 'salary'\n",
    "    , 'company_total_employees'\n",
    "    , 'company_rating'\n",
    "    , 'company_total_review'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ba2c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil scrap job ke 1\n",
      "Berhasil scrap job ke 2\n",
      "Berhasil scrap job ke 3\n",
      "Berhasil scrap job ke 4\n",
      "Berhasil scrap job ke 5\n",
      "Berhasil scrap job ke 6\n",
      "try again 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 21\u001b[0m\n\u001b[0;32m     20\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m job_detail_box \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//div[@data-automation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjobDetailsPage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     22\u001b[0m job_role \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//h1[@data-automation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob-detail-title\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:778\u001b[0m, in \u001b[0;36mWebDriver.find_elements\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# Return empty list if driver returns null\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# See https://github.com/SeleniumHQ/selenium/issues/4555\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENTS, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:352\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    350\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\urllib3\\_request_methods.py:118\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m    119\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m    120\u001b[0m     )\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\urllib3\\_request_methods.py:217\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    215\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m     response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32mE:\\anaconda_setup\\installation\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m retrial \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtry again \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retrial \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "button_next_clicked = 0\n",
    "indexing = 0\n",
    "next_available = 1\n",
    "\n",
    "while next_available:\n",
    "    \n",
    "    try:\n",
    "        list_jobs_box2 = driver.find_elements(By.XPATH, \"//article[.//div[@data-testid='company-logo-container'] and .//a[@data-automation='job-list-item-link-overlay' and @data-testid='job-list-item-link-overlay']]\")\n",
    "        # list_jobs_box2 = list_jobs_box.find_elements(By.XPATH, \".//a[@data-testid='job-card-title']\")\n",
    "\n",
    "        for job_ind, job_box_ind in enumerate(list_jobs_box2):\n",
    "\n",
    "            retrial = 0\n",
    "            while retrial<3:\n",
    "                try:\n",
    "                    # job_box_ind.click()\n",
    "                    objek_click = job_box_ind.find_elements(By.XPATH, \".//a[@data-automation='jobTitle']\")[0]\n",
    "                    objek_click.click()\n",
    "                    # list_jobs_box2[0].find_elements(By.XPATH, \".//a[@data-automation='jobTitle']\")[0].click()\n",
    "                    time.sleep(0.8)\n",
    "                    job_detail_box = driver.find_elements(By.XPATH, \"//div[@data-automation='jobDetailsPage']\")[0]\n",
    "                    job_role = driver.find_elements(By.XPATH, \"//h1[@data-automation='job-detail-title']\")[0].text\n",
    "                    break\n",
    "                except:\n",
    "                    retrial +=1\n",
    "                    print(f'try again {retrial}')\n",
    "                    time.sleep(1)\n",
    "                    if retrial >= 3:\n",
    "                        continue\n",
    "\n",
    "            # job_company = driver.find_elements(By.XPATH, \"//div[@class='y735df0 _1iz8dgsgi _1iz8dgs5a _1iz8dgsg2 _1iz8dgsfi _1akoxc52e _1akoxc52n']\")[0]\n",
    "            # job_company2 = job_company.find_elements(By.XPATH, \".//div[@class='y735df0 _1iz8dgsr _1iz8dgsem _1iz8dgsej _1iz8dgsba _1iz8dgsb7 _1iz8dgs4y _1iz8dgsfm']\")[0]\n",
    "            job_company_final = job_box_ind.find_elements(By.XPATH, \".//a[@data-automation='jobCompany']\")[0].text\n",
    "\n",
    "            try:\n",
    "                job_rating_final = driver.find_elements(By.XPATH, \".//span[@data-automation='company-review']\")[0].text\n",
    "                job_total_review = driver.find_elements(By.XPATH, \".//a[@data-automation='job-header-company-review-link']\")[0].text\n",
    "            except:\n",
    "                job_rating_final = np.nan\n",
    "                job_total_review = np.nan\n",
    "\n",
    "            job_location = driver.find_elements(By.XPATH, \"//span[@data-automation='job-detail-location']\")[0].text\n",
    "            job_industries = driver.find_elements(By.XPATH, \"//span[@data-automation='job-detail-classifications']\")[0].text\n",
    "            job_employment_type = driver.find_elements(By.XPATH, \"//span[@data-automation='job-detail-work-type']\")[0].text\n",
    "\n",
    "            job_duration = driver.find_elements(By.XPATH, \".//span[contains(text(), 'Posted')]\")[0].text\n",
    "\n",
    "            job_description = driver.find_elements(By.XPATH, \"//div[@data-automation='jobAdDetails']\")[0].text\n",
    "            try:\n",
    "                job_salary = driver.find_elements(By.XPATH, \"//span[@data-automation='job-detail-salary']\")[0].text\n",
    "            except:\n",
    "                job_salary = np.nan\n",
    "\n",
    "            try:\n",
    "                profile_box = driver.find_elements(By.XPATH, \"//div[@data-automation='company-profile']\")[0]\n",
    "                total_employee = profile_box.find_elements(By.XPATH, \".//span[contains(text(), 'employees')]\")[0].text\n",
    "\n",
    "                if 'employ' not in total_employee:\n",
    "                    total_employee = np.nan\n",
    "            except:\n",
    "                total_employee = np.nan\n",
    "\n",
    "            job_url = driver.current_url\n",
    "            job_id = jobid_getter(job_url)\n",
    "\n",
    "            job_db2.loc[indexing, 'company'] = job_company_final\n",
    "            job_db2.loc[indexing, 'role'] = job_role\n",
    "            job_db2.loc[indexing, 'location'] = job_location\n",
    "            job_db2.loc[indexing, 'post_duration'] = job_duration\n",
    "            job_db2.loc[indexing, 'total_application'] = np.nan\n",
    "            job_db2.loc[indexing, 'level'] = np.nan\n",
    "            job_db2.loc[indexing, 'employment_status'] = job_employment_type\n",
    "            job_db2.loc[indexing, 'job_function'] = np.nan\n",
    "            job_db2.loc[indexing, 'industries'] = job_industries\n",
    "            job_db2.loc[indexing, 'job_description'] = job_description\n",
    "            job_db2.loc[indexing, 'url'] = job_url\n",
    "            job_db2.loc[indexing, 'job_id'] = job_id\n",
    "            job_db2.loc[indexing, 'salary'] = job_salary\n",
    "            job_db2.loc[indexing, 'company_total_employees'] = total_employee\n",
    "            job_db2.loc[indexing, 'company_rating'] = job_rating_final\n",
    "            job_db2.loc[indexing, 'company_total_review'] = job_total_review\n",
    "\n",
    "            print(f'Berhasil scrap job ke {indexing+1}')\n",
    "            indexing += 1\n",
    "\n",
    "        try:\n",
    "            cek_next = driver.find_elements(By.XPATH, \".//a[@aria-label = 'Selanjutnya']\")[0]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            next_available = 0\n",
    "        \n",
    "        cek_next.click()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        next_available = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f123f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_days(time_str):\n",
    "    match = re.search(r'(\\d+|\\d+\\+?) (menit|jam|hari)', time_str)\n",
    "    if not match:\n",
    "        return None\n",
    "    value, unit = match.groups()\n",
    "    value = int(value.replace('+', ''))\n",
    "    if unit == 'jam':\n",
    "        return value / 24\n",
    "    elif unit == 'hari':\n",
    "        return value\n",
    "    elif unit == 'menit':\n",
    "        return value / 1440\n",
    "    return None\n",
    "\n",
    "# Function to replace specific word\n",
    "def replace_word(text, target_word, replacement_word):\n",
    "    # Compile the pattern with case insensitivity\n",
    "    pattern = re.compile(re.escape(target_word), re.IGNORECASE)\n",
    "    # Replace the target word with the replacement word\n",
    "    return pattern.sub(replacement_word, text)\n",
    "\n",
    "def find_matches(text, pattern, watchlist):\n",
    "    matches = pattern.findall(text)\n",
    "    # Map matches to their original case in the watchlist\n",
    "    original_matches = {match.upper(): match for match in watchlist}\n",
    "    matched_words = [original_matches[match.upper()] for match in matches]\n",
    "    \n",
    "    # Remove duplicates and return a comma-separated string\n",
    "    return ', '.join(sorted(set(matched_words), key=lambda x: text.upper().index(x.upper())))\n",
    "\n",
    "def contains_keywords(text):\n",
    "    # Convert text to uppercase for case-insensitive comparison\n",
    "    text_upper = text.upper()\n",
    "    \n",
    "    # Check for exclusion keywords first\n",
    "    if pattern_exp_exclusion.search(text_upper):\n",
    "        return False\n",
    "    \n",
    "    # Check for English keyword combinations\n",
    "    primary_match_en = pattern_exp_en.search(text_upper)\n",
    "    year_match_en = year_pattern_en.search(text_upper)\n",
    "    \n",
    "    # Check for Indonesian keyword combinations\n",
    "    primary_match_id = pattern_exp_id.search(text_upper)\n",
    "    year_match_id = year_pattern_id.search(text_upper)\n",
    "    \n",
    "    # Return True if the correct combinations are found\n",
    "    if primary_match_en and year_match_en:\n",
    "        return True\n",
    "    if primary_match_id and year_match_id:\n",
    "        return True\n",
    "    \n",
    "    # Return False if only partial matches are found\n",
    "    if primary_match_en and not year_match_en:\n",
    "        return False\n",
    "    if primary_match_id and not year_match_id:\n",
    "        return False\n",
    "\n",
    "    # Default to False if no conditions are met\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d63be697",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_db2['posted_since_days_ago'] = job_db2['post_duration'].apply(lambda x: convert_to_days(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1400216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_db2['data_scraped_at'] = today_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e171ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_analyst = job_db2[\n",
    "    (job_db2['role'].str.upper().str.contains('ANAL', na=False))\n",
    "    | (job_db2['role'].str.upper().str.contains('BI DEV', na=False))\n",
    "    | (job_db2['role'].str.upper().str.contains('INTELLIGENCE', na=False))\n",
    "    | (job_db2['role'].str.upper().str.contains('INSIGHT', na=False))\n",
    "    | (job_db2['role'].str.upper().str.contains('POWER BI', na=False))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "job_data_analyst_final = job_data_analyst[\n",
    "    (~job_data_analyst['role'].str.upper().str.contains('SOFTWARE', na=False)) \n",
    "    & (~job_data_analyst['role'].str.upper().str.contains('SECURITY', na=False))\n",
    "    & (~job_data_analyst['role'].str.upper().str.contains('IT BUSINESS ANALYST', na=False))\n",
    "    & (~job_data_analyst['role'].str.upper().str.contains('SYSTEM ANALYST', na=False))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329d6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tools = [\n",
    "    'CANVA', 'CAMTASIA', 'SAP', 'EXCEL', 'VISIO', 'POWERBI', 'POWER BI', 'TABLEAU', 'LOOKER STUDIO', 'LOOKER', \n",
    "    'POWER POINT', 'VISUAL BASIC', 'MACRO', 'QLIKSENSE', 'PYTHON', 'BIGQUERY', 'BIG QUERY', 'AWS', 'AWS ATHENA', \n",
    "    'A/B TEST', ' R ', 'PHYTON', 'DBT', 'GITLAB', 'GITHUB', 'GOOGLE CLOUD', 'NOSQL', 'MYSQL', 'POSTGRESQL', 'SQL', \n",
    "    'G-SUITE', 'GOOGLE ADS', 'META ADS', 'SQL SERVER', '.NET', 'PPT', 'POWERPOINT', 'GOOGLE DATA STUDIO', 'AIRFLOW', \n",
    "    'HIVE', 'JUPYTER', 'HTML', 'CSS', 'JAVASCRIPT', 'JAVA', 'STATA', 'REDSHIFT', 'ORACLE', 'HADOOP', 'AZURE', 'SCALA', \n",
    "    'C++', 'CI/CD', 'SELENIUM', 'KATALON', 'SPSS', 'GCP', 'APACHE SPARK', 'SPARK', 'VBA', 'PENTAHO', 'MONGODB', 'DAX', \n",
    "    'HDFS', 'SSIS', 'SSAS', 'SSRS', 'TENSORFLOW', 'PYTORCH'\n",
    "]\n",
    "\n",
    "list_hardskills = [\n",
    "    'BIG DATA', 'DATA CLEANSING', 'DATA WAREHOUSE', 'DATA VISUALIZATION', 'BI TOOLS', 'DATA AUTOMATION', 'DATA MINING', \n",
    "    'DATA MANAGEMENT', 'LMS', 'TECHNICAL GUIDANCE', 'BRD', 'FSD', 'TSD', 'UML', 'MS. OFFICE', 'CLUSTERING', 'REGRESSION', \n",
    "    'CLASSIFICATION', 'DATA MODEL', 'DATA MANIPULATION', 'DASHBOARD', 'ETL', 'DESCRIPTIVE', 'PREDICTIVE', 'DATA PROFILING', \n",
    "    'DATA PIPELINE', 'HLOOKUP', 'VLOOKUP', 'PIVOT', 'DATAMART', 'DATA MART', 'DATA EXTRACTION', 'MACHINE LEARNING', \n",
    "    'INFERENTIAL STATISTIC', 'COMPUTER VISION', 'NATURAL LANGUAGE PROCESSING', 'NLP', 'FINANCIAL REPORT', 'AB TEST', ' LLM ', \n",
    "    'SCRAPING', 'WEB CRAWL', 'OLAP', 'DATA BESAR', 'DATA COLLECTION', 'FRAUD'\n",
    "]\n",
    "\n",
    "list_language = [\n",
    "    'ENGLISH', 'MANDARIN', 'BAHASA'\n",
    "]\n",
    "\n",
    "list_exp_en = ['YEARS IN', 'EXPERIENCE', 'EXP']\n",
    "list_exp_id = ['PENGALAMAN']\n",
    "year_keyword_en = ['YEAR']\n",
    "year_keyword_id = ['TAHUN']\n",
    "list_exp_exclusion = ['GRADUATE', 'STUDENT', 'LESS THAN 1 YEAR']\n",
    "\n",
    "# Compile the regex pattern\n",
    "pattern_tools = re.compile('|'.join([re.escape(word) for word in list_tools]), re.IGNORECASE)\n",
    "pattern_hardskills = re.compile('|'.join([re.escape(word) for word in list_hardskills]), re.IGNORECASE)\n",
    "pattern_exp_en = re.compile('|'.join(list_exp_en), re.IGNORECASE)\n",
    "pattern_exp_id = re.compile('|'.join(list_exp_id), re.IGNORECASE)\n",
    "year_pattern_en = re.compile('|'.join(year_keyword_en), re.IGNORECASE)\n",
    "year_pattern_id = re.compile('|'.join(year_keyword_id), re.IGNORECASE)\n",
    "pattern_exp_exclusion = re.compile('|'.join(list_exp_exclusion), re.IGNORECASE)\n",
    "pattern_lang = re.compile('|'.join(list_language), re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98e050cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOLS DETECTION\n",
    "job_data_analyst_final['tools_mentioned'] = job_data_analyst_final['job_description'].apply(lambda x: find_matches(x, pattern_tools, list_tools))\n",
    "job_data_analyst_final['tools_mentioned'] = job_data_analyst_final['tools_mentioned'].apply(lambda x: replace_word(x, 'PHYTON', 'PYTHON'))\n",
    "job_data_analyst_final['tools_mentioned'] = job_data_analyst_final['tools_mentioned'].apply(lambda x: replace_word(x, 'POWERBI', 'POWER BI'))\n",
    "job_data_analyst_final['tools_mentioned'] = job_data_analyst_final['tools_mentioned'].apply(lambda x: replace_word(x, 'BIG QUERY', 'BIGQUERY'))\n",
    "\n",
    "# HARD SKILL DETECTION\n",
    "job_data_analyst_final['hardskills_mentioned'] = job_data_analyst_final['job_description'].apply(lambda x: find_matches(x, pattern_hardskills, list_hardskills))\n",
    "\n",
    "# EXPERIENCE REQ DETECTION\n",
    "job_data_analyst_final['need_experience'] = job_data_analyst_final['job_description'].apply(contains_keywords)\n",
    "\n",
    "# LANGUAGE DETECTION\n",
    "job_data_analyst_final['language_mentioned'] = job_data_analyst_final['job_description'].apply(lambda x: find_matches(x, pattern_lang, list_language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9236ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_analyst_final['platform'] = 'Jobstreet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ec08890",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_analyst_final['data_scraped_at'] = today_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "544adf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_analyst_final.to_excel(f'{today_string}_jobstreet_data_analyst.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c57514",
   "metadata": {},
   "source": [
    "## Hasil crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5827fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34a48b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_linkedin = glob.glob('*linkedin_data_analyst.xlsx')\n",
    "da_jobstreet = glob.glob('*jobstreet_data_analyst.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd54208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_linkedin = pd.read_excel(da_linkedin[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7ddf1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_all_aggregate = pd.concat([job_data_analyst_final, job_linkedin], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02315c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Function to calculate the actual post date\n",
    "def calculate_post_date(days_ago):\n",
    "    return current_date - timedelta(days=days_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1bc07ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>role</th>\n",
       "      <th>location</th>\n",
       "      <th>post_duration</th>\n",
       "      <th>total_application</th>\n",
       "      <th>job_description</th>\n",
       "      <th>level</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>job_function</th>\n",
       "      <th>industries</th>\n",
       "      <th>...</th>\n",
       "      <th>company_total_employees</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>company_total_review</th>\n",
       "      <th>posted_since_days_ago</th>\n",
       "      <th>data_scraped_at</th>\n",
       "      <th>tools_mentioned</th>\n",
       "      <th>hardskills_mentioned</th>\n",
       "      <th>need_experience</th>\n",
       "      <th>language_mentioned</th>\n",
       "      <th>platform</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [company, role, location, post_duration, total_application, job_description, level, employment_status, job_function, industries, url, job_id, salary, company_total_employees, company_rating, company_total_review, posted_since_days_ago, data_scraped_at, tools_mentioned, hardskills_mentioned, need_experience, language_mentioned, platform]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_all_aggregate[job_all_aggregate['posted_since_days_ago'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0afad4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the 'posted_since_days_ago' column\n",
    "job_all_aggregate['post_date'] = job_all_aggregate['posted_since_days_ago'].apply(calculate_post_date)\n",
    "\n",
    "# Format the date to 'YYYY-mm-dd'\n",
    "job_all_aggregate['post_date'] = job_all_aggregate['post_date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57182cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_range(salary_str):\n",
    "    if pd.isna(salary_str):\n",
    "        return np.nan, np.nan\n",
    "    salary_str = salary_str.replace('Rp', '').replace('per month', '').replace('–', '-').replace('.', '').replace(',', '').replace('IDR', '')\n",
    "    min_salary, max_salary = salary_str.split('-')\n",
    "    return int(min_salary.strip()), int(max_salary.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "855f9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_all_aggregate[['min_salary', 'max_salary']] = job_all_aggregate['salary'].apply(lambda x: pd.Series(extract_salary_range(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a23cfb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_job = job_all_aggregate[['job_id', 'company', 'role', 'post_date']]\n",
    "master_job.to_excel('master_date_jobpost.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd7dd8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the scope of the application\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "#credentials to the account\n",
    "cred = ServiceAccountCredentials.from_json_keyfile_name('D:/7. Project/nibble-scraping-e8bfcb2c2f4a.json',scope)\n",
    "\n",
    "# authorize the clientsheet\n",
    "client = gspread.authorize(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a77f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scrap = job_all_aggregate.values.tolist()\n",
    "\n",
    "# Access an existing Google Sheet by title\n",
    "spreadsheet = client.open('job_crawler_2024')\n",
    "\n",
    "# Select the first worksheet of the spreadsheet\n",
    "worksheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "# Upload the data\n",
    "worksheet.clear()  # Clear existing data (optional)\n",
    "set_with_dataframe(worksheet, job_all_aggregate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
